{"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1mV9sAY4QBKLmS58dpFGHgwCXQKRASR31","timestamp":1714038079272},{"file_id":"1LOwhFY5rFzA2f3O2ZaY8vn3oh_nuqzGy","timestamp":1691595101338},{"file_id":"1Zmaceu65d7w4Tcd-cfnZRb6k_Tcv2b8g","timestamp":1691590111735},{"file_id":"1CSSeSBs4ki99r2LI5d3cT6qvszfB2VfQ","timestamp":1691513246004},{"file_id":"1iRqQyDAbnS0qYTajA_uVmv96lBz5N65n","timestamp":1691511454214},{"file_id":"1Y1YzhGDDeE1D1BOYCXAEsIU_WqTNVVSB","timestamp":1691507164920}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8255171,"sourceType":"datasetVersion","datasetId":4898857}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data generation step","metadata":{"id":"Way3_PuPpIuE"}},{"cell_type":"markdown","source":"**Write your prompt here. Make it as descriptive as possible!**\n\n**Then, choose the temperature (between 0 and 1) to use when generating data. Lower values are great for precise tasks, like writing code, whereas larger values are better for creative tasks, like writing stories.**\n\n**Finally, choose how many examples you want to generate. The more you generate, a) the longer it takes and b) the more expensive data generation will be. But generally, more examples will lead to a higher-quality model. 100 is usually the minimum to start.**","metadata":{"id":"lY-3DvlIpVSl"}},{"cell_type":"code","source":"prompt = \"\"\"\nYou are a model that knows all and every Topic about HIGH SCHOOL BIOLOGY, excluding the Evolution Topic, and responds with a well-reasoned, step-by-step thought out response in English, Explaining and simplifing the information in way that a 10 YEAR OLD or STRUGGING TEENAGER would understand, also giving definitions of the confusing biological names of complicated words, names and phrases. \nDO NOT answer any other questions unless they are biology questions.\nDO NOT repeat the prompt or the words in the prompt, rather just introduce yourself and answer only HIGH SCHOOL BIOLOGY questions.\nREMEMBER to be kind and respectful and use emojis to show some emotion.\nREMEMBER to explain in way a 10 year old would best understand.\n\"\"\"\ntemperature = 0.4\nnumber_of_examples = 75","metadata":{"id":"R7WKZyxtpUPS","executionInfo":{"status":"ok","timestamp":1714111698505,"user_tz":-120,"elapsed":452,"user":{"displayName":"Manoah “MANFOX” Chama","userId":"11663014587381923936"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run this to generate the dataset.","metadata":{"id":"1snNou5PrIci"}},{"cell_type":"code","source":"!pip install openai","metadata":{"id":"zuL2UaqlsmBD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"sk-a6b7a1abf1014ae486c92067aa2e8823\", base_url=\"https://api.deepseek.com/v1\")","metadata":{"id":"sDF2HgCLrtLp","executionInfo":{"status":"ok","timestamp":1714109373787,"user_tz":-120,"elapsed":1365,"user":{"displayName":"Manoah “MANFOX” Chama","userId":"11663014587381923936"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_example(prompt, prev_examples, temperature=0.5):\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n        }\n    ]\n\n    if len(prev_examples) > 0:\n        if len(prev_examples) > 10:\n            prev_examples = random.sample(prev_examples, 10)\n        for example in prev_examples:\n            messages.append({\n                \"role\": \"assistant\",\n                \"content\": example\n            })\n\n    response = client.chat.completions.create(\n        model=\"deepseek-chat\",\n        messages=messages,\n        temperature=temperature,\n        max_tokens=1000,\n    )\n\n    return response.choices[0].message.content\n\n# Generate examples\nprev_examples = []\nfor i in range(number_of_examples):\n    print(f'Generating example {i}')\n    example = generate_example(prompt, prev_examples, temperature)\n    prev_examples.append(example)\n\nprint(prev_examples)","metadata":{"id":"Rdsd82ngpHCG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We also need to generate a system message.","metadata":{"id":"KC6iJzXjugJ-"}},{"cell_type":"code","source":"def generate_system_message(prompt):\n\n    response = client.chat.completions.create(\n        model=\"deepseek-chat\",\n        messages=[\n          {\n            \"role\": \"system\",\n            \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n          },\n          {\n              \"role\": \"user\",\n              \"content\": prompt.strip(),\n          }\n        ],\n        temperature=temperature,\n        max_tokens=500,\n    )\n\n    return response.choices[0].message.content\n\nsystem_message = generate_system_message(prompt)\n\nprint(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')","metadata":{"id":"xMcfhW6Guh2E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714113253578,"user_tz":-120,"elapsed":5937,"user":{"displayName":"Manoah “MANFOX” Chama","userId":"11663014587381923936"}},"outputId":"adc4a575-bf2d-48a0-db75-ba1b360c29bd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now let's put our examples into a dataframe and turn them into a final pair of datasets.","metadata":{"id":"G6BqZ-hjseBF"}},{"cell_type":"code","source":"import pandas as pd\n\n# Initialize lists to store prompts and responses\nprompts = []\nresponses = []\n\n# Parse out prompts and responses from examples\nfor example in prev_examples:\n  try:\n    split_example = example.split('-----------')\n    prompts.append(split_example[1].strip())\n    responses.append(split_example[3].strip())\n  except:\n    pass\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'prompt': prompts,\n    'response': responses\n})\n\n# Remove duplicates\ndf = df.drop_duplicates()\n\nprint('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n\ndf.head()","metadata":{"id":"7CEdkYeRsdmB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split into train and test sets.","metadata":{"id":"A-8dt5qqtpgM"}},{"cell_type":"code","source":"# Split the data into train and test sets, with 90% in the train set\ntrain_df = df.sample(frac=0.9, random_state=42)\ntest_df = df.drop(train_df.index)\n\n# Save the dataframes to .jsonl files\ntrain_df.to_json('train.jsonl', orient='records', lines=True)\ntest_df.to_json('test.jsonl', orient='records', lines=True)","metadata":{"id":"GFPEn1omtrXM","executionInfo":{"status":"ok","timestamp":1714113294141,"user_tz":-120,"elapsed":558,"user":{"displayName":"Manoah “MANFOX” Chama","userId":"11663014587381923936"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Install necessary libraries","metadata":{"id":"AbrFgrhG_xYi"}},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers trl==0.4.7","metadata":{"id":"jZCUJZxpuv3z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"id":"lPG7wEPetFx2","executionInfo":{"status":"ok","timestamp":1714113536419,"user_tz":-120,"elapsed":22052,"user":{"displayName":"Manoah “MANFOX” Chama","userId":"11663014587381923936"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Hyperparameters","metadata":{"id":"moVo0led-6tu"}},{"cell_type":"code","source":"model_name = \"NousResearch/Meta-Llama-3-8B\" # use this if you have access to the official LLaMA 2 model \"meta-llama/Llama-2-7b-chat-hf\", though keep in mind you'll need to pass a Hugging Face key argument\ndataset_name = \"/kaggle/working/train.jsonl\"\nnew_model = \"llama3-tutormodelv01\"\naccess_token = \"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\"\nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\noutput_dir = \"./results\"\nnum_train_epochs = 11\nfp16 = False\nbf16 = False\nper_device_train_batch_size = 4\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 1\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"constant\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 25\nlogging_steps = 5\nmax_seq_length = None\npacking = False\ndevice_map = {\"\": 0}","metadata":{"id":"bqfbhUZI-4c_","executionInfo":{"status":"ok","timestamp":1714117739568,"user_tz":-120,"elapsed":658,"user":{"displayName":"Manoah “MANFOX” Chama","userId":"11663014587381923936"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Datasets and Train","metadata":{"id":"F-J5p5KS_MZY"}},{"cell_type":"code","source":"# Load datasets\ntrain_dataset = load_dataset('json', data_files='/kaggle/working/train.jsonl', split=\"train\")\nvalid_dataset = load_dataset('json', data_files='/kaggle/working/test.jsonl', split=\"train\")\n\n# Preprocess datasets\ntrain_dataset_mapped = train_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\nvalid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map,\n    token=access_token\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"all\",\n    evaluation_strategy=\"steps\",\n    eval_steps=5  # Evaluate every 20 steps\n)\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset_mapped,\n    eval_dataset=valid_dataset_mapped,  # Pass validation dataset here\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\ntrainer.train()\ntrainer.model.save_pretrained(new_model)","metadata":{"id":"qf1qxbiF-x6p","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference","metadata":{"id":"F6fux9om_c4-"}},{"cell_type":"code","source":"from transformers import pipeline\n\nprompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n [/INST]\" # replace the command here with something relevant to your task\nnum_new_tokens = 100  # change to the number of new tokens you want to generate\n\n# Count the number of tokens in the prompt\nnum_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n\n# Calculate the maximum length for the generation\nmax_length = num_prompt_tokens + num_new_tokens\n\ngen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\nresult = gen(prompt)\nprint(result[0]['generated_text'].replace(prompt, ''))","metadata":{"id":"7hxQ_Ero2IJe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714114688040,"user_tz":-120,"elapsed":33352,"user":{"displayName":"Manoah “MANFOX” Chama","userId":"11663014587381923936"}},"outputId":"5af29aa8-5902-4129-d121-ce8e085492ab","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Merge the model and store in Google Drive","metadata":{"id":"Ko6UkINu_qSx"}},{"cell_type":"code","source":"# Cloud Storage\nfrom google.cloud import storage\nstorage_client = storage.Client(project='TutorGPT')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drive.mount('/content/drive')\n\nmodel_path = \"/kaggle/working/tutorchatmodel-01\"  # change to your preferred path\n\n# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Save the merged model\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)","metadata":{"id":"AgKCL7fTyp9u","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load a fine-tuned model from Drive and run inference","metadata":{"id":"do-dFdE5zWGO"}},{"cell_type":"code","source":"from google.colab import drive\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndrive.mount('/content/drive')\n\nmodel_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to the path where your model is saved\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"id":"xg6nHPsLzMw-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\nprompt = \"What is 2 + 2?\"  # change to your desired prompt\ngen = pipeline('text-generation', model=model, tokenizer=tokenizer)\nresult = gen(prompt)\nprint(result[0]['generated_text'])","metadata":{"id":"fBK2aE2KzZ05"},"execution_count":null,"outputs":[]}]}